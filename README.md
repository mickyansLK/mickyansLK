
# Hi, I‚Äôm Michael Ansah üëã

## About Me
I‚Äôm a **Data Engineer** specialising in building **scalable, production-grade ELT/ETL pipelines**, robust **data models**, and **analytics infrastructure** that deliver measurable business value.

My work focuses on transforming raw, fragmented data into **reliable, analytics-ready datasets** that enable decision-making across marketing, finance, and operations.

I enjoy working across the full data lifecycle ‚Äî from ingestion and orchestration to modelling, analytics, and stakeholder enablement.

---

## Tech Stack
**Languages:** Python, SQL  
**Data Engineering:** dbt, Apache Spark, Kafka, Prefect, Airflow  
**Data Platforms:** BigQuery, Snowflake, Databricks, Microsoft Fabric  
**Analytics & BI:** Power BI, Tableau  
**DevOps & Tooling:** Docker, GitHub Actions, CI/CD  

---

## Select Highlighted Projects
A selection of end-to-end data engineering projects demonstrating real-world use cases, production-ready design, and business impact.

---

### Customer 360 ELT Pipeline
**Summary:**  
Designed and built a scalable ELT platform integrating GA4, Bloomreach, and financial data into BigQuery, with analytics-ready transformations managed using dbt.

**Business Value:**  
Delivered a unified customer view for marketing and finance teams, enabling consistent cross-domain reporting and reducing data latency for downstream analytics.

**What I Did:**  
- Built ingestion pipelines using Python  
- Modelled analytics-ready datasets with dbt  
- Enabled self-service reporting via Power BI  

**Technologies:** Python, BigQuery, dbt, Power BI  
**Repository:** https://github.com/mickyansLK/customer_360  

---

### Streaming Ship Telemetry Platform
**Summary:**  
Implemented a real-time data ingestion and processing pipeline for vessel telemetry using Kafka and Spark, with analytical storage in DuckDB.

**Business Value:**  
Enabled near real-time operational monitoring and analytics, supporting faster insight into vessel performance and behavioural patterns.

**What I Did:**  
- Designed a streaming ingestion architecture  
- Processed high-velocity event data with Apache Spark  
- Optimised analytical queries using DuckDB  

**Technologies:** Kafka, Apache Spark, DuckDB, Python  
**Repository:** https://github.com/mickyansLK/ship-telemetry-project  

---

### Insurance Analytics & Policy Data Platform
**Summary:**  
Built an end-to-end analytics platform for insurance policy, claims, and customer data, transforming raw operational datasets into analytics-ready models.

**Business Value:**  
Improved visibility into policy performance and claims trends, supporting data-driven underwriting, pricing, and risk assessment decisions.

**What I Did:**  
- Modelled core insurance entities (policies, claims, customers)  
- Implemented transformation logic using dbt  
- Enabled reporting and insight generation through BI dashboards  

**Technologies:** Python, SQL, dbt, BigQuery, Power BI  
**Repository:** https://github.com/mickyansLK/insure_4_all

---

## How I Work
- Focus on **clarity, reliability, and scalability**  
- Strong emphasis on **data modelling and analytics enablement**  
- Prefer **simple, well-documented architectures** over unnecessary complexity  
- Build with both **engineers and business users** in mind  

---

## Contact
üì´ **LinkedIn:** https://linkedin.com/in/michael-ansah-data  
üìç **Location:** London, UK  
üìå **Open to:** Data Engineering & Analytics Infrastructure roles  

---

*This GitHub profile is intended to function as a living portfolio ‚Äî showcasing not just code, but how data engineering creates business impact.*

